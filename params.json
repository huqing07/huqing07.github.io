{"name":"Huqing07.GitHub.io","tagline":"","body":"---\r\ntitle: \"Practical Machine Learning Project\"\r\nauthor: \"Qing\"\r\ndate: \"March 21, 2015\"\r\noutput: html_document\r\n---\r\n##Introduction\r\nA predictive model has been built by using machine learning algorithm on Weight Lifting Exercises (WLE) dataset (source: http://groupware.les.inf.puc-rio.br/har). The WLE training dataset has 19622 observations and 160 variables. They are the data from accelerometers on the belt, forearm, arm, and dumbell of six young health participants. The participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different ways: exactly according to the specification (A), throwing the elbows to the front (B), lifting the dumbbell only halfway (C), lowering the dumbbell only halfway (D) and throwing the hips to the front (E). The objective of this project is to predict the manner (\"classe\" variable in the dataset) in which they did the exercise and apply the model to a 20-observation test dataset.\r\n\r\n##Load the datasets \r\n```{r}\r\ntrain <- read.csv('~/Desktop/pml-training.csv', header = TRUE)\r\ntest <- read.csv('~/Desktop/pml-testing.csv', header = TRUE)\r\n```\r\n\r\n##Build a predictive model using a machine learning algorithm - gbm\r\nHere gradient boosting machine (gbm) is selected for predicting \"classe\". This is because gbm inherits all the good featuress of trees: 1) can easily ignore redundant variabls; 2) can handle mixed predictors (quantitative and qualitative); 3) can handle missing data elegantly through surrogate splits. In addition, it also improves on the weak features via slowly growing small trees and typically leads to better performance.\r\n\r\n5-fold cross validation is used to select the best number of trees. From the deviance plot we can see cross validation error (green curve) is very close to training error (black curve). As a result, we may expect the out of sample error to be close to the cross validation error. The model accuracy on the whole training dataset is 99.5%.\r\n\r\n```{r}\r\nlibrary(gbm)\r\ntrain.1 <- train[-c(1, 3, 4, 5)] ##remove the obs index and timestamp variables\r\n\r\nmodgbm <- gbm(classe~., \r\n              data = train.1,\r\n              cv.folds = 5,\r\n              distribution = 'multinomial',\r\n              interaction.depth = 3,\r\n              shrinkage = .1,\r\n              n.trees = 150,\r\n              bag.fraction = 1,\r\n              train.fraction = 1)\r\n\r\ngbm.perf(modgbm, method = 'cv')\r\n\r\npred.train <- predict(modgbm, train.1, type = 'response')\r\npred.train.cat<-NULL\r\nfor (i in 1:nrow(train.1)){\r\n    pred.train.cat[i] <- LETTERS[which(pred.train[i, , 1] == max(pred.train[i, , 1]))]\r\n}\r\naccuracy <- mean(pred.train.cat == train.1$classe)\r\naccuracy\r\n```\r\n\r\n##Predict \"classe\" on test dataset\r\nApply the gbm model to predict the 20-observation test dataset. \r\n```{r}\r\nnew.test <- rbind(train[1, -160], test[-160]) \r\nnew.test.1 <- new.test[-c(1, 3, 4, 5)]\r\n\r\npred.test <- predict(modgbm, new.test.1, n.trees = 150, type = 'response')\r\n\r\npred.test.cat<-NULL\r\nfor (i in 1:21){\r\npred.test.cat[i] <- LETTERS[which(pred.test[i, , 1] == max(pred.test[i, , 1]))]\r\n}\r\npred.test.cat.1 <- pred.test.cat[2:21]\r\n\r\npml_write_files = function(x){\r\n    n = length(x)\r\n    for(i in 1:n){\r\n        filename = paste0(\"problem_id_\",i,\".txt\")\r\n        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)\r\n    }\r\n}\r\npml_write_files(pred.test.cat.1)\r\n```\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}